import os
import sys
import time
import warnings
import logging
import numpy as np
from tqdm import tqdm

# ------------------- 1. æ ¸å¿ƒ GPU ä¸æ¡†æ¶è®¾ç½® (å¿…é¡»æ”¾åœ¨æœ€å‰é¢) -------------------
# è®¾ç½® segmentation_models ä½¿ç”¨ tf.keras (å…³é”®ï¼è§£å†³ keras å’Œ tf.keras å†²çª)
os.environ["SM_FRAMEWORK"] = "tf.keras"

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ç¡®ä¿è¿™é‡Œæ˜¯ä½ çš„æ˜¾å¡ID
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras import mixed_precision

# ---- GPU æ˜¾å­˜æŒ‰éœ€å¢é•¿ & æ··åˆç²¾åº¦è®¾ç½® ----
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for g in gpus:
            tf.config.experimental.set_memory_growth(g, True)
        print(f"âœ… å‘ç° GPU: {gpus}")
        print("âœ… å·²å¼€å¯æ˜¾å­˜æŒ‰éœ€åˆ†é…")
    except RuntimeError as e:
        print(e)

# å¼€å¯æ··åˆç²¾åº¦ (Mixed Precision) - æå¤§æå‡ 30/40 ç³»æ˜¾å¡é€Ÿåº¦
# try:
#     policy = mixed_precision.Policy('mixed_float16')
#     mixed_precision.set_global_policy(policy)
#     print("âœ… å·²å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ (mixed_float16)")
# except Exception as e:
#     print("âš ï¸ æ··åˆç²¾åº¦å¼€å¯å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤ç²¾åº¦:", e)
# --------------------------------------------------------------------------

# ------------------- Clean warnings -------------------
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
np.seterr(all="ignore")
logging.getLogger("tensorflow").setLevel(logging.ERROR)
tf.get_logger().setLevel("ERROR")
# ------------------------------------------------------

from numpy.random import seed

# ==== ç»Ÿä¸€æ”¹ç”¨ tf.keras ====
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,
    TensorBoard, CSVLogger
)
from tensorflow.keras.optimizers import Adam

# Our internal functions and libraries
from models.models import ResearchModels
from utils.data import DataSet
from utils.data_aug import DataSet_aug


# ----------------- metrics & losses -------------------
def dice_coef(y_true, y_pred):
    smooth = 1e-5
    # Flatten
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])

    # æ··åˆç²¾åº¦ä¸‹ï¼Œç¡®ä¿è®¡ç®—ä½¿ç”¨ float32 ä»¥é¿å…æ•°å€¼æº¢å‡º
    y_true_f = tf.cast(y_true_f, tf.float32)
    y_pred_f = tf.cast(y_pred_f, tf.float32)

    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)


def dice_coef_loss(y_true, y_pred):
    return 1.0 - dice_coef(y_true, y_pred)


def iou_coef(y_true, y_pred):
    smooth = 1e-5
    # å¼ºåˆ¶è½¬æ¢ä¸º float32 è¿›è¡Œè®¡ç®—
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])
    union = K.sum(y_true, [1, 2, 3]) + K.sum(y_pred, [1, 2, 3]) - intersection
    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)
    return iou


def iou_loss(y_true, y_pred):
    return 1.0 - iou_coef(y_true, y_pred)


# ------------------------------------------------------

# ================= Experiment Settings =================
model_name = 'LRDNet_Test_SM_TF2'  # é¿å…å˜é‡åè¦†ç›– model å¯¹è±¡
augmentation = False
save_best_only = True
seeding = False
batch_size = 8  # GPU ä¸Šé€šå¸¸å¯ä»¥æ¯” CPU å¼€å¤§ä¸€ç‚¹ï¼Œå¦‚æœæ˜¯ 4090 å¯ä»¥å°è¯• 8 æˆ– 16
patience = 15
epochs = 1500
save_models = True

if seeding:
    seedi = 100
    seed(seedi)
    tf.random.set_seed(seedi)

# æ ¹æ®æ¨¡å‹åé€‰å°ºå¯¸
if 'LRDNet' in model_name:
    width, height = 1280, 384
    print('************** Using Size 1280 x 384 **************')
if 'SM' in model_name:
    width, height = 256, 256
    print('************** Using Size 256 x 256 **************')

# æ•°æ®ç®¡é“
if augmentation:
    train_images = DataSet_aug(model=model_name, target='train', batch_size=batch_size, width=width, height=height)
    val_images = DataSet_aug(model=model_name, target='valid', batch_size=batch_size, width=width, height=height)
    aug = '[AUGBIG]'
else:
    train_images = DataSet(model=model_name, target='train', batch_size=batch_size, width=width, height=height)
    val_images = DataSet(model=model_name, target='valid', batch_size=batch_size, width=width, height=height)
    aug = ''

steps_per_epoch = train_images.steps_per_epoch
validation_steps = val_images.validation_steps
train_data = train_images.td
val_data = val_images.vd

# ç›®å½•
checkpoints_dir = os.path.join('results', model_name)
model_dir = os.path.join(checkpoints_dir, model_name + '_Weights')
os.makedirs(checkpoints_dir, exist_ok=True)
os.makedirs(model_dir, exist_ok=True)

# å›è°ƒ
ckpt_name = os.path.join(model_dir, model_name + aug + '.({epoch:03d})-[{iou_coef:.4f}]-[{val_iou_coef:.4f}].hdf5')
checkpointer = ModelCheckpoint(
    filepath=ckpt_name,
    verbose=1,
    save_best_only=save_best_only,
    monitor='val_iou_coef',
    mode='max'
)

tb = TensorBoard(log_dir=os.path.join(checkpoints_dir, model_name + '_logs', model_name))
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-7)
timestamp = time.time()
csv_logger = CSVLogger(os.path.join(checkpoints_dir, model_name + '_logs', f'{model_name}{timestamp}.log'))
early_stopper = EarlyStopping(monitor='val_iou_coef', mode='max', patience=patience, verbose=1)

callbacks = [tb, csv_logger, checkpointer, reduce_lr]
if not save_models:
    callbacks.append(early_stopper)

# æ¨¡å‹æ„å»º
# æ³¨æ„ï¼šResearchModels å†…éƒ¨ä¼šå› ä¸º SM_FRAMEWORK çš„è®¾ç½®è€Œæ­£ç¡®è°ƒç”¨ tf.keras
rm = ResearchModels(modelname=model_name, height=height, width=width)

# ç¼–è¯‘æ¨¡å‹
# è°ƒæ•´ï¼šæ··åˆç²¾åº¦ä¸‹ï¼Œepsilon éœ€è¦è°ƒæ•´ä»¥ä¿æŒç¨³å®šæ€§ï¼Œä½†åœ¨ Adam é»˜è®¤å‚æ•°é€šå¸¸æ²¡é—®é¢˜
rm.model.compile(optimizer=Adam(learning_rate=5e-5), loss=iou_loss, metrics=[iou_coef])

print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: Batch Size = {batch_size}, Epochs = {epochs}")

history = rm.model.fit(
    train_data,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    verbose=1,
    callbacks=callbacks,
    validation_data=val_data,
    validation_steps=validation_steps
)
