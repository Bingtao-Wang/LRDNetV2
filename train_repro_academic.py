# ==============================================================================
# æ–‡ä»¶å: train_repro_academic.py
# åˆ›å»ºç›®çš„:
#   1. LRDNet è®ºæ–‡ Baseline å¤ç°ä¸»ç¨‹åºã€‚
#   2. åŒ…å«å­¦æœ¯çº§è¯„ä¼°æŒ‡æ ‡ (Precision, Recall, F1-Score)ã€‚
#   3. ä¿®å¤äº† OOM é—®é¢˜ (è°ƒæ•´ Batch Size)ã€‚
#   4. ä½¿ç”¨ mixed_float16 è¿›è¡ŒåŠ é€Ÿã€‚
# ==============================================================================

import os
import sys
import time
import logging
import numpy as np
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras import mixed_precision
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,
    TensorBoard, CSVLogger
)
from tensorflow.keras.optimizers import Adam


# ==== å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å— ====
# å¿…é¡»å…ˆè®¾ç½®ç¯å¢ƒå˜é‡
os.environ["SM_FRAMEWORK"] = "tf.keras"
from models.models import ResearchModels
from utils.data_repro_pipeline import AcademicDataGenerator  # å¯¼å…¥æ–°æ•°æ®ç®¡é“

# ------------------- GPU ä¸ æ··åˆç²¾åº¦è®¾ç½® -------------------
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# æ˜¾å­˜æŒ‰éœ€åˆ†é…
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for g in gpus:
            tf.config.experimental.set_memory_growth(g, True)
    except RuntimeError as e:
        print(e)

# å¼€å¯æ··åˆç²¾åº¦
# try:
#     policy = mixed_precision.Policy('mixed_float16')
#     mixed_precision.set_global_policy(policy)
#     print(f"âœ… Compute Policy: {policy.compute_dtype}")
# except Exception as e:
#     print(f"âš ï¸ Mixed Precision Failed: {e}")


# ------------------- å­¦æœ¯è¯„ä¼°æŒ‡æ ‡ (Metrics) -------------------
def precision_m(y_true, y_pred):
    # è°ƒå¤§ epsilon é¿å…é™¤é›¶ï¼Œä½†åœ¨ BS=1 æ—¶ä¸è¦ç”¨å¤ªå¤§çš„ smooth æ©ç›–é”™è¯¯
    smooth = 1e-7
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    # é˜ˆå€¼äºŒå€¼åŒ– (Thresholding) - è¿™ä¸€ç‚¹å¾ˆå…³é”®ï¼
    # ä¹‹å‰çš„è®¡ç®—æ˜¯ç”¨æ¦‚ç‡å€¼ç›´æ¥ä¹˜ï¼Œè¿™å« Soft Metricã€‚
    # å­¦æœ¯è¯„ä¼°é€šå¸¸ç”¨ Hard Metric (å…ˆ >0.5 å˜æˆ 0/1 å†ç®—)ã€‚
    y_pred_hard = K.cast(y_pred > 0.5, tf.float32)

    true_positives = K.sum(K.round(K.clip(y_true * y_pred_hard, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred_hard, 0, 1)))

    return true_positives / (predicted_positives + smooth)


def recall_m(y_true, y_pred):
    smooth = 1e-7
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    y_pred_hard = K.cast(y_pred > 0.5, tf.float32)

    true_positives = K.sum(K.round(K.clip(y_true * y_pred_hard, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))

    return true_positives / (possible_positives + smooth)


def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2 * ((precision * recall) / (precision + recall + 1e-7))


def iou_coef(y_true, y_pred):
    # è¿™æ˜¯ä¸€ä¸ª Soft IoUï¼Œç”¨äº Loss è®¡ç®—æ˜¯ OK çš„ï¼Œä½†ç”¨äºè¯„ä¼°æœ‰ç‚¹è™š
    # ä½†ä¸ºäº†ä¿æŒ Loss å¯å¾®ï¼Œæˆ‘ä»¬è¿™é‡Œçš„å®ç°ç”¨äº Lossï¼Œ
    # æˆ‘ä»¬å¯ä»¥å•ç‹¬å†™ä¸€ä¸ª iou_score ç”¨äº Metrics
    smooth = 1e-7  # å¤§å¹…å‡å° Smooth
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])
    union = K.sum(y_true, [1, 2, 3]) + K.sum(y_pred, [1, 2, 3]) - intersection

    # å¦‚æœ Union æ˜¯ 0 (å³ Ground Truth å…¨é»‘ ä¸” é¢„æµ‹å…¨é»‘)ï¼ŒIoU åº”è¯¥æ˜¯ 1
    # åˆ©ç”¨ tf.where å¤„ç†è¿™ç§æƒ…å†µ
    iou = (intersection + smooth) / (union + smooth)

    return K.mean(iou, axis=0)


# ä¸“é—¨ç”¨äºæ˜¾ç¤ºçš„ Hard IoU (è¯„ä¼°ç”¨)
def iou_metric(y_true, y_pred):
    y_pred_hard = K.cast(y_pred > 0.5, tf.float32)
    y_true = tf.cast(y_true, tf.float32)

    intersection = K.sum(K.abs(y_true * y_pred_hard), axis=[1, 2, 3])
    union = K.sum(y_true, [1, 2, 3]) + K.sum(y_pred_hard, [1, 2, 3]) - intersection

    smooth = 1e-7
    iou = (intersection + smooth) / (union + smooth)
    return K.mean(iou, axis=0)


# Loss ä¾ç„¶ä½¿ç”¨ Soft IoU ä»¥ä¿è¯æ¢¯åº¦å¹³æ»‘
def iou_loss(y_true, y_pred):
    return 1.0 - iou_coef(y_true, y_pred)


# ------------------- å®éªŒå‚æ•°é…ç½® -------------------
MODEL_NAME = 'LRDNet_Academic_Repro_v2'
# ã€å…³é”®ã€‘æ˜¾å­˜ä¿®å¤ï¼šå¯¹äº 1280x384 + VGG19ï¼ŒRTX 4060Ti (16GB) å»ºè®® Batch Size = 4
# å¦‚æœä¾ç„¶æŠ¥é”™ OOMï¼Œè¯·æ”¹ä¸º 2
BATCH_SIZE = 1
WIDTH, HEIGHT = 1280, 384
EPOCHS = 1500
LR = 1e-4

print(f"ğŸš€ Starting Academic Experiment: {MODEL_NAME}")
print(f"ğŸ“ Input Size: {WIDTH}x{HEIGHT} | Batch Size: {BATCH_SIZE}")

# ------------------- æ•°æ®ç®¡é“åˆå§‹åŒ– -------------------
train_gen = AcademicDataGenerator(target='train', batch_size=BATCH_SIZE, width=WIDTH, height=HEIGHT)
val_gen = AcademicDataGenerator(target='valid', batch_size=BATCH_SIZE, width=WIDTH, height=HEIGHT)

train_data = train_gen.get_dataset()
val_data = val_gen.get_dataset()

# ------------------- æ¨¡å‹æ„å»ºä¸ç¼–è¯‘ -------------------
rm = ResearchModels(modelname=MODEL_NAME, height=HEIGHT, width=WIDTH)

# åœ¨æ¨¡å‹ç¼–è¯‘å¤„ä¿®æ”¹
rm.model.compile(
    optimizer=Adam(learning_rate=LR),
    loss=iou_loss,
    metrics=[iou_metric, f1_m, precision_m, recall_m]
)


# ------------------- Callbacks è®¾ç½® -------------------
checkpoints_dir = os.path.join('results', MODEL_NAME)
os.makedirs(checkpoints_dir, exist_ok=True)
model_weights_dir = os.path.join(checkpoints_dir, 'weights')
os.makedirs(model_weights_dir, exist_ok=True)

# ç›‘æ§ val_f1_m (å­¦æœ¯ç•Œæ›´çœ‹é‡ F1)
ckpt_path = os.path.join(model_weights_dir, 'best_model.hdf5')
checkpointer = ModelCheckpoint(
    filepath=ckpt_path,
    verbose=1,
    save_best_only=True,
    monitor='val_f1_m',  # ä¿å­˜ F1 åˆ†æ•°æœ€é«˜çš„æ¨¡å‹
    mode='max'
)

csv_logger = CSVLogger(os.path.join(checkpoints_dir, 'training_log.csv'))
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1, min_lr=1e-6)
early_stop = EarlyStopping(monitor='val_f1_m', mode='max', patience=50, verbose=1)

callbacks = [csv_logger, checkpointer, reduce_lr, early_stop]

# ------------------- å¼€å§‹è®­ç»ƒ -------------------
try:
    history = rm.model.fit(
        train_data,
        steps_per_epoch=len(train_gen),
        epochs=EPOCHS,
        verbose=1,
        callbacks=callbacks,
        validation_data=val_data,
        validation_steps=len(val_gen)
    )
except KeyboardInterrupt:
    print("\nğŸ›‘ Training interrupted by user.")
except Exception as e:
    print(f"\nâŒ An error occurred during training: {e}")
    print("ğŸ’¡ å»ºè®®: å¦‚æœæ˜¯ OOM é”™è¯¯ï¼Œè¯·å°è¯•å°† train_repro_academic.py ä¸­çš„ BATCH_SIZE æ”¹ä¸º 2")
